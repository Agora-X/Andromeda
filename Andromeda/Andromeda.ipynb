{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1fk6k-IAXPW",
        "outputId": "7a61856a-0e76-40b1-d16e-81c4945eb6bc"
      },
      "outputs": [],
      "source": [
        "#andromeda with multi query attention\n",
        "!git clone https://github.com/kyegomez/Optimus-Prime.git\n",
        "%cd Optimus-Prime\n",
        "!pip install --upgrade torch\n",
        "!pip install -r requirements.txt\n",
        "!pip install einops\n",
        "# !pip install --upgrade torch\n",
        "\n",
        "# %cd Optimus-Prime\n",
        "# # %cd examples\n",
        "# # !ls\n",
        "# !python3 trainandromeda.py \n",
        "# #%cd enwik8_simple\n",
        "# # !python trainandromeda.py\n",
        "\n",
        "\n",
        "from torch.serialization import load\n",
        "import torch \n",
        "from x_transformers import TransformerWrapper, Decoder, AutoregressiveWrapper\n",
        "\n",
        "#training\n",
        "import random\n",
        "import tqdm\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# from torchmetrics import MetricCollection, Accuracy\n",
        "\n",
        "\n",
        "# constants\n",
        "\n",
        "NUM_BATCHES = int(1e5)\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATE_EVERY = 4\n",
        "LEARNING_RATE = 1e-4\n",
        "VALIDATE_EVERY  = 100\n",
        "GENERATE_EVERY  = 500\n",
        "GENERATE_LENGTH = 1024\n",
        "SEQ_LEN = 1024\n",
        "SAVE_EVERY=500\n",
        "\n",
        "\n",
        "# helpers\n",
        "\n",
        "def cycle(loader):\n",
        "    while True:\n",
        "        for data in loader:\n",
        "            yield data\n",
        "\n",
        "def decode_token(token):\n",
        "    return str(chr(max(32, token)))\n",
        "\n",
        "def decode_tokens(tokens):\n",
        "    return ''.join(list(map(decode_token, tokens)))\n",
        "\n",
        "model = TransformerWrapper(\n",
        "    num_tokens=64007,\n",
        "    max_seq_len=8192,\n",
        "    use_abs_pos_emb = False,\n",
        "    attn_layers = Decoder(\n",
        "        dim=512,\n",
        "        depth=6,\n",
        "        heads=8,\n",
        "        alibi_pos_bias=True,\n",
        "        alibi_num_heads=4,\n",
        "        rotary_xpos=True,\n",
        "        attn_flash = True,\n",
        "        deepnorm=True,\n",
        "        shift_tokens=1,\n",
        "        attn_one_kv_head = True,\n",
        "        #qk_norm=True\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "model = AutoregressiveWrapper(model)\n",
        "model.cuda()\n",
        "\n",
        "with gzip.open('./enwik8.gz') as file:\n",
        "  data = np.frombuffer(file.read(int(95e6)), dtype=np.uint8).copy()\n",
        "  train_x, valid_x = np.split(data, [int(90e6)])\n",
        "  data_train, data_val = torch.from_numpy(train_x), torch.from_numpy(valid_x) #.cuda()??\n",
        "\n",
        "class TextSamplerDataset(Dataset):\n",
        "    def __init__(self, data, seq_len):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n",
        "        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n",
        "        return full_seq.cuda()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.seq_len\n",
        "\n",
        "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\n",
        "val_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\n",
        "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE, drop_last = True))\n",
        "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE, drop_last = True))\n",
        "\n",
        "# optimizer\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# training\n",
        "\n",
        "# #init tensorboard \n",
        "# writer = SummaryWriter(log_dir=\"./log\")\n",
        "\n",
        "# #define metrics\n",
        "# metrics = MetricCollection({'accuracy': Accuracy(num_classes=num_classes, task='classification')})\n",
        "device=\"cuda\"\n",
        "for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
        "    model.train()\n",
        "\n",
        "    for _ in range(GRADIENT_ACCUMULATE_EVERY):\n",
        "        loss = model(next(train_loader))#.to(device)\n",
        "        (loss / GRADIENT_ACCUMULATE_EVERY).backward()#.to(device)#.cuda()\n",
        "\n",
        "    print(f'training loss: {loss.item()}')\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "    if i % VALIDATE_EVERY == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                loss = model(next(val_loader))\n",
        "                print(f'validation loss: {loss.item()}')\n",
        "\n",
        "                # # Calculate validation metrics\n",
        "                # val_metrics = MetricCollection({'val_accuracy': Accuracy()})\n",
        "                # val_metrics(loss, model(next(val_loader)).argmax(dim=-1))\n",
        "\n",
        "                # # Add validation metrics to the SummaryWriter\n",
        "                # writer.add_scalar('Validation/Accuracy', val_metrics['val_accuracy'].compute(), global_step=i)\n",
        "\n",
        "    if i % GENERATE_EVERY == 0:\n",
        "        model.eval()\n",
        "        inp = random.choice(val_dataset)[:-1]\n",
        "        prime = decode_tokens(inp)\n",
        "        print(f'%s \\n\\n %s', (prime, '*' * 100))\n",
        "\n",
        "        sample = model.generate(inp, GENERATE_LENGTH)\n",
        "        output_str = decode_tokens(sample)\n",
        "        print(output_str)\n",
        "\n",
        "    # Save the model every save_every iterations\n",
        "    if i % SAVE_EVERY == 0:\n",
        "        # Specify the directory and filename to save the model\n",
        "        save_dir = './saved_models/'\n",
        "        save_filename = 'model_checkpoint.pt'\n",
        "\n",
        "        # Create the save directory if it doesn't exist\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # Save the model checkpoint\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, save_filename))\n",
        "        print(f\"Model saved at iteration {i}\")\n",
        "\n",
        "#     # Add training metrics to the SummaryWriter\n",
        "#     writer.add_scalar('Training/Accuracy', metrics['accuracy'].compute(), global_step=i)\n",
        "\n",
        "#     # Close the SummaryWriter\n",
        "# writer.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
